{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9de61af2-5627-4fcd-bac1-52679e58d85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "692a6df7-41cc-47aa-8b26-b4f62a2e7ab7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize an empty list to store paper details\n",
    "papers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64f8ad39-506c-460b-900a-d9dbc4d99414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through multiple pages (0 to 390, in steps of 10)\n",
    "for i in range(0, 400, 10):\n",
    "    url = f\"https://scholar.google.com/scholar?start={i}&q=Self-healing+materials+in+construction&hl=en&as_sdt=0,5\"\n",
    "    \n",
    "    # Fetch the page\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find the container that holds the articles\n",
    "    article_container = soup.find_all('div', {'class': 'gs_ri'})\n",
    "   \n",
    "    # Loop through each article and extract information    \n",
    "    for article in article_container:\n",
    "        title = article.find('h3').text if article.find('h3') else 'N/A'\n",
    "        author_info = article.find('div', {'class': 'gs_a'}).text if article.find('div', {'class': 'gs_a'}) else 'N/A'\n",
    "        abstract = article.find('div', {'class': 'gs_rs'}).text if article.find('div', {'class': 'gs_rs'}) else 'N/A'\n",
    "        \n",
    "        papers.append({\n",
    "            'Title': title,\n",
    "            'Author Info': author_info,\n",
    "            'Abstract': abstract\n",
    "        })\n",
    "        \n",
    "    # Sleep for 5 seconds to respect Google Scholar's terms\n",
    "    time.sleep(5)\n",
    "    \n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df = pd.DataFrame(papers)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('shmin_google_scholar.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70e8cb1-594c-48ca-bbff-f1eec64b5929",
   "metadata": {},
   "source": [
    "In Google Scholar, each search result page typically shows 10 papers. So, if want to scrape information from the first 40 pages of search results, need to loop through 0, 10, 20, ..., 390, each representing the starting index of the papers on each search result page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0aff3ed-50a5-4236-a78c-84d5007bf646",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
